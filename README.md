# Causality-for-Mechanistic-Interpretability
This repository features research papers focused on achieving mechanistic interpretability through causality. By leveraging causal reasoning, these studies aim to unravel how specific components in AI models contribute to decision-making processes, enhancing transparency and reliability in AI systems.
Contend

# Table of Contents: Causal Interpretability with Different Intervention Methods

-  [1.Patch-Based Methods](#patch-based-methods)
   - [1.1. Activation Patching](##Activation-Patching)
   - [1.2. Path Patching](##Path-Patching)
   - [1.3. Feature Patching](##Feature-Patching)
   - [1.4. Global Activation Patching](##Global-Activation-Patching)
- [2. Ablation-Based Methods](#ablation-based-methods)
   - [2.1. Neuron Ablation](##Neuron-Ablation)
   - [2.2. Path Ablation](##Path-Ablation)
   - [2.3. Layer Ablation](##Layer-Ablation)
## Patch-Based Methods
Patching methods aim to analyze whether a component has the same impact in two scenarios by replacing the activation values, paths, or feature representations of a certain component in the model.
### Activation Patching
1. (arXiv 20024) **Towards Best Practices of Activation Patching in Language Models: Metrics and Methods.** _Bereska_. [[pdf](https://arxiv.org/abs/2309.16042)]
### Path Patching

### Feature Patching

### Global Activation Patching

## Ablation-Based Methods
Ablation methods evaluate the contribution of a component to the model's output by deleting or disabling that component.
### Neuron Ablation

### Path Ablation

### Layer Ablation

